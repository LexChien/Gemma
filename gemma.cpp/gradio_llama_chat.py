# gradio_llama_chat.py - Gemma Chatbot v1.6 æ­£å¼ç‰ˆ

import gradio as gr
import requests
import time
import os
import json

API_URL = "http://localhost:8080/completion"
LOG_PATH = "llama-server.log"
PARAM_CONFIG_PATH = "chat_config.json"
LANGUAGE_CONFIG_PATH = "lang_config.json"

headers = {"Content-Type": "application/json"}

# --- å¤šèªè¨€æ–‡å­—è³‡æ–™ ---
def get_texts(language):
    if language == "ä¸­æ–‡":
        return {
            "title": "ğŸ¦™ Gemma èŠå¤©æ©Ÿå™¨äºº",
            "input_label": "è¼¸å…¥å•é¡Œâ€¦",
            "log_label": "ğŸ” llama-server æ—¥èªŒ (æœ€å¾Œ 20 è¡Œ)",
            "refresh_log": "ğŸ”„ é‡æ–°æ•´ç† Log",
            "clear_chat": "æ¸…é™¤å°è©±",
            "param_title": "ğŸ”§ æ¨ç†åƒæ•¸",
            "advance_param": "ğŸ”¬ é€²éšåƒæ•¸ï¼ˆå¯é¸ï¼‰",
            "preset_chat": "ğŸ’¬ èŠå¤©æ¨¡å¼",
            "preset_creative": "ğŸ¨ å‰µä½œæ¨¡å¼",
            "preset_coding": "ğŸ’» ç¨‹å¼æ¨¡å¼",
            "save_config": "ğŸ’¾ å„²å­˜åƒæ•¸çµ„åˆ",
            "load_config": "ğŸ“‚ è¼‰å…¥åƒæ•¸",
            "status_label": "ç‹€æ…‹é€šçŸ¥",
        }
    elif language == "æ—¥æœ¬èª":
        return {
            "title": "ğŸ¦™ Gemma ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ",
            "input_label": "è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„â€¦",
            "log_label": "ğŸ” llama-server ãƒ­ã‚° (æœ€å¾Œã®20è¡Œ)",
            "refresh_log": "ğŸ”„ ãƒ­ã‚°ã‚’æ›´æ–°",
            "clear_chat": "ãƒãƒ£ãƒƒãƒˆã‚’ã‚¯ãƒªã‚¢",
            "param_title": "ğŸ”§ æ¨è«–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿",
            "advance_param": "ğŸ”¬ é«˜åº¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰",
            "preset_chat": "ğŸ’¬ ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰",
            "preset_creative": "ğŸ¨ ã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰",
            "preset_coding": "ğŸ’» ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰",
            "save_config": "ğŸ’¾ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¿å­˜",
            "load_config": "ğŸ“‚ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª­è¾¼",
            "status_label": "ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹",
        }
    elif language == "í•œêµ­ì–´":
        return {
            "title": "ğŸ¦™ Gemma ì±—ë´‡",
            "input_label": "ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”â€¦",
            "log_label": "ğŸ” llama-server ë¡œê·¸ (ë§ˆì§€ë§‰ 20ì¤„)",
            "refresh_log": "ğŸ”„ ë¡œê·¸ ìƒˆë¡œê³ ì¹¨",
            "clear_chat": "ëŒ€í™” ì‚­ì œ",
            "param_title": "ğŸ”§ ì¶”ë¡  ë§¤ê°œë³€ìˆ˜",
            "advance_param": "ğŸ”¬ ê³ ê¸‰ ë§¤ê°œë³€ìˆ˜ (ì„ íƒì‚¬í•­)",
            "preset_chat": "ğŸ’¬ ì±„íŒ… ëª¨ë“œ",
            "preset_creative": "ğŸ¨ ì°½ì‘ ëª¨ë“œ",
            "preset_coding": "ğŸ’» ì½”ë”© ëª¨ë“œ",
            "save_config": "ğŸ’¾ ë§¤ê°œë³€ìˆ˜ ì €ì¥",
            "load_config": "ğŸ“‚ ë§¤ê°œë³€ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°",
            "status_label": "ìƒíƒœ",
        }
    else:  # English
        return {
            "title": "ğŸ¦™ Gemma Chatbot",
            "input_label": "Enter your question...",
            "log_label": "ğŸ” llama-server Log (Last 20 lines)",
            "refresh_log": "ğŸ”„ Refresh Log",
            "clear_chat": "Clear Chat",
            "param_title": "ğŸ”§ Inference Parameters",
            "advance_param": "ğŸ”¬ Advanced Parameters (Optional)",
            "preset_chat": "ğŸ’¬ Chat Mode",
            "preset_creative": "ğŸ¨ Creative Mode",
            "preset_coding": "ğŸ’» Coding Mode",
            "save_config": "ğŸ’¾ Save Parameters",
            "load_config": "ğŸ“‚ Load Parameters",
            "status_label": "Status",
        }

def get_system_prompt(language):
    if language == "ä¸­æ–‡":
        return "ä½ æ˜¯ä¸€ä½è°æ˜ä¸”è¬™è™›çš„èªè¨€æ¨¡å‹åŠ©ç†ï¼Œè«‹ç”¨ç¹é«”ä¸­æ–‡ä½œç­”ã€‚"
    elif language == "æ—¥æœ¬èª":
        return "ã‚ãªãŸã¯è³¢ãã¦è¬™è™šãªè¨€èªãƒ¢ãƒ‡ãƒ«ã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚"
    elif language == "í•œêµ­ì–´":
        return "ë‹¹ì‹ ì€ ë˜‘ë˜‘í•˜ê³  ê²¸ì†í•œ ì–¸ì–´ ëª¨ë¸ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. í•œêµ­ì–´ë¡œ ëŒ€ë‹µí•˜ì‹­ì‹œì˜¤."
    else:
        return "You are a helpful and humble language model assistant. Please respond in English."

# --- èªè¨€è¨­å®š ---
def save_language(lang, path=LANGUAGE_CONFIG_PATH):
    try:
        with open(path, "w", encoding="utf-8") as f:
            json.dump({"language": lang}, f)
    except:
        pass

def load_language(path=LANGUAGE_CONFIG_PATH):
    if os.path.exists(path):
        try:
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
            return data.get("language", "English")
        except:
            return "English"
    else:
        return "English"

# --- llama äº’å‹•æ ¸å¿ƒ ---
def chat_with_llama(user_input, history, language, temperature, repetition_penalty, max_tokens, top_p, top_k, stop_str, repeat_last_n, seed, mirostat, mirostat_tau, mirostat_eta):
    system_prompt = get_system_prompt(language)
    prompt = system_prompt + "\n"

    for i in range(0, len(history), 2):
        if i + 1 < len(history):
            prompt += f"User: {history[i]['content']}\nAssistant: {history[i+1]['content']}\n"
    prompt += f"User: {user_input}\nAssistant:"

    stop_tokens = [s.strip() for s in stop_str.split("|") if s.strip()]

    payload = {
        "prompt": prompt,
        "n_predict": max_tokens,
        "temperature": temperature,
        "repeat_penalty": repetition_penalty,
        "top_p": top_p,
        "top_k": int(top_k),
        "stop": stop_tokens,
        "repeat_last_n": int(repeat_last_n),
        "seed": int(seed),
        "mirostat": int(mirostat),
        "mirostat_tau": float(mirostat_tau),
        "mirostat_eta": float(mirostat_eta)
    }

    try:
        response = requests.post(API_URL, headers=headers, json=payload, timeout=120)
        response.raise_for_status()
        result = response.json()["content"]
        return result
    except Exception as e:
        return f"[Error] {e}"

# --- æ—¥èªŒå’ŒèŠå¤©ç´€éŒ„ ---
def read_recent_log(lines=20):
    if not os.path.exists(LOG_PATH):
        return "[Log file not found]"
    with open(LOG_PATH, "r", encoding="utf-8", errors="ignore") as f:
        return "".join(f.readlines()[-lines:])

def save_chat_history(history, language):
    lang_suffix = {
        "English": "en",
        "ä¸­æ–‡": "zh",
        "æ—¥æœ¬èª": "ja",
        "í•œêµ­ì–´": "ko",
    }.get(language, "en")

    path = f"chat_logs_{lang_suffix}.jsonl"
    try:
        with open(path, "a", encoding="utf-8") as f:
            f.write(json.dumps({"timestamp": time.time(), "history": history}, ensure_ascii=False) + "\n")
    except:
        pass

# âœ… ä¿®æ­£ç¼ºå¤±çš„ save_params / load_params å®šç¾©

def save_params(temp, rp, max_tok, tp, tk, stop, rln, seed, miro, tau, eta, path=PARAM_CONFIG_PATH):
    try:
        config = {
            "temperature": temp,
            "repetition_penalty": rp,
            "max_tokens": max_tok,
            "top_p": tp,
            "top_k": tk,
            "stop": stop,
            "repeat_last_n": rln,
            "seed": seed,
            "mirostat": miro,
            "mirostat_tau": tau,
            "mirostat_eta": eta
        }
        with open(path, "w", encoding="utf-8") as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        return "âœ… åƒæ•¸å·²å„²å­˜"
    except Exception as e:
        return f"âŒ å„²å­˜å¤±æ•—: {e}"

def load_params(path=PARAM_CONFIG_PATH):
    try:
        with open(path, "r", encoding="utf-8") as f:
            config = json.load(f)
        return (
            config.get("temperature", 0.7),
            config.get("repetition_penalty", 1.1),
            config.get("max_tokens", 256),
            config.get("top_p", 0.9),
            config.get("top_k", 40),
            config.get("stop", "User:|Assistant:"),
            config.get("repeat_last_n", 64),
            config.get("seed", 42),
            config.get("mirostat", 0),
            config.get("mirostat_tau", 5.0),
            config.get("mirostat_eta", 0.1),
        )
    except Exception as e:
        print(f"[Load Error] {e}")
        return (0.7, 1.1, 256, 0.9, 40, "User:|Assistant:", 64, 42, 0, 5.0, 0.1)

# with gr.Blocks(title="Gemma Chatbot") as demo:
with gr.Blocks(
    title="Gemma Chatbot",
    css="""
    footer {
        display: none !important;
        visibility: hidden !important;
        height: 0px !important;
        padding: 0 !important;
        margin: 0 !important;
    }
    """
) as demo:
    default_language = load_language()
    language_selector = gr.Dropdown(["English", "ä¸­æ–‡", "æ—¥æœ¬èª", "í•œêµ­ì–´"], value=default_language, label="ğŸŒ Language")
    texts = get_texts(default_language)

    title_markdown = gr.Markdown(f"# {texts['title']}")

    with gr.Row():
        chatbot = gr.Chatbot(type='messages')
        with gr.Column():
            log_display = gr.Textbox(label=texts['log_label'], lines=20)
            refresh_log_btn = gr.Button(texts['refresh_log'])

    msg = gr.Textbox(label=texts['input_label'])
    clear = gr.Button(texts['clear_chat'])

    with gr.Accordion(texts['param_title'], open=False) as param_accordion:
        temperature = gr.Slider(0.1, 1.5, value=0.7, label="Temperature")
        repetition_penalty = gr.Slider(0.8, 2.0, value=1.1, label="Repetition Penalty")
        max_tokens = gr.Slider(16, 2048, value=256, step=16, label="Max New Tokens")
        top_p = gr.Slider(0.1, 1.0, value=0.9, label="Top-p")
        top_k = gr.Slider(0, 100, value=40, step=1, label="Top-k")

        advance_param_markdown = gr.Markdown(texts['advance_param'])
        stop = gr.Textbox(value="User:|Assistant:", label="Stop Tokens")
        repeat_last_n = gr.Number(value=64, precision=0, label="Repeat Last N")
        seed = gr.Number(value=42, precision=0, label="Seed")
        mirostat = gr.Dropdown([0, 1, 2], value=0, label="Mirostat")
        mirostat_tau = gr.Number(value=5.0, precision=1, label="Mirostat Tau")
        mirostat_eta = gr.Number(value=0.1, precision=2, label="Mirostat Eta")

        with gr.Row():
            preset_chat = gr.Button(texts['preset_chat'])
            preset_creative = gr.Button(texts['preset_creative'])
            preset_coding = gr.Button(texts['preset_coding'])

        with gr.Row():
            save_config = gr.Button(texts['save_config'])
            load_config = gr.Button(texts['load_config'])
            save_msg = gr.Textbox(label=texts['status_label'], interactive=False)

    state = gr.State([])

    # --- æ ¸å¿ƒäº’å‹• ---
    def user_chat(user_input, history, language, temp, rp, max_tok, tp, tk, stop, rln, seed, miro, tau, eta):
        bot_response = chat_with_llama(user_input, history, language, temp, rp, max_tok, tp, tk, stop, rln, seed, miro, tau, eta)
        history.append({"role": "user", "content": user_input})
        history.append({"role": "assistant", "content": bot_response})
        save_chat_history(history, language)
        return history, "", read_recent_log(), history

    # --- åˆ‡æ›èªè¨€ ---
    def update_language(lang):
        save_language(lang)
        texts = get_texts(lang)
        return (
            gr.update(value=f"# {texts['title']}"),
            gr.update(label=texts['input_label']),
            gr.update(label=texts['log_label']),
            gr.update(value=texts['refresh_log']),
            gr.update(value=texts['clear_chat']),
            gr.update(label=texts['param_title']),
            gr.update(value=texts['advance_param']),
            gr.update(value=texts['preset_chat']),
            gr.update(value=texts['preset_creative']),
            gr.update(value=texts['preset_coding']),
            gr.update(value=texts['save_config']),
            gr.update(value=texts['load_config']),
            gr.update(label=texts['status_label']),
        )

    # --- Event ç¶å®š ---
    msg.submit(user_chat, [msg, state, language_selector, temperature, repetition_penalty, max_tokens, top_p, top_k, stop, repeat_last_n, seed, mirostat, mirostat_tau, mirostat_eta],
               [chatbot, msg, log_display, state])
    clear.click(lambda: ([], "", read_recent_log(), []), None, [chatbot, msg, log_display, state])
    refresh_log_btn.click(lambda: read_recent_log(), None, log_display)

    preset_chat.click(lambda: (0.7, 1.1, 256, 0.9, 40), None, [temperature, repetition_penalty, max_tokens, top_p, top_k])
    preset_creative.click(lambda: (1.3, 1.0, 512, 0.95, 50), None, [temperature, repetition_penalty, max_tokens, top_p, top_k])
    preset_coding.click(lambda: (0.3, 1.2, 512, 0.85, 20), None, [temperature, repetition_penalty, max_tokens, top_p, top_k])

    save_config.click(lambda temp, rp, max_tok, tp, tk, stop, rln, seed, miro, tau, eta: save_params(temp, rp, max_tok, tp, tk, stop, rln, seed, miro, tau, eta),
                      [temperature, repetition_penalty, max_tokens, top_p, top_k, stop, repeat_last_n, seed, mirostat, mirostat_tau, mirostat_eta], save_msg)

    load_config.click(lambda: load_params(), None, [temperature, repetition_penalty, max_tokens, top_p, top_k, stop, repeat_last_n, seed, mirostat, mirostat_tau, mirostat_eta])

    language_selector.change(update_language, inputs=[language_selector], outputs=[
        title_markdown, msg, log_display, refresh_log_btn, clear,
        param_accordion, advance_param_markdown,
        preset_chat, preset_creative, preset_coding,
        save_config, load_config, save_msg
    ])

if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7860)
